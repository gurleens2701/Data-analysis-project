{\rtf1\ansi\ansicpg1252\cocoartf2758
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;
\red0\green0\blue0;\red200\green205\blue211;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985;
\cssrgb\c0\c1\c1;\cssrgb\c82383\c83963\c85905;\cssrgb\c100000\c100000\c99941;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid1\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\cf2 \
\pard\pardeftab720\sa400\partightenfactor0

\fs32 \cf3 \cb4 \expnd0\expndtw0\kerning0
Initially, a logistic regression model will be trained to predict the likelihood of survey respondents being obese, leveraging their responses to survey questions. Subsequently, three distinct wrapper methods will be employed to identify a more compact feature subset.\
\pard\pardeftab720\partightenfactor0
\cf3 \cb4 Sequential Forward Selection, Sequential Backward Floating Selection, and Recursive Feature Elimination will be utilized. Following the implementation of each wrapper method, the model accuracy will be assessed on the resulting reduced feature subsets. This evaluation will be compared with the model accuracy achieved when utilizing all available features.\
\
In Simple terms:\
\
\pard\pardeftab720\sa400\partightenfactor0
\cf5 \cb2 The provided code is part of a data analysis project that aims to predict whether survey respondents are obese based on their eating habits and physical condition. The project involves the following steps:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf5 \kerning1\expnd0\expndtw0 		\cf5 \expnd0\expndtw0\kerning0
Logistic Regression Model:
\f0\b0 \cf5 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
A logistic regression model is trained using data from a survey. The dataset includes various predictor variables (features) such as age, gender, eating habits, physical activity, etc.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
The logistic regression model is evaluated for accuracy, resulting in a score of approximately 76.6%.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf5 \kerning1\expnd0\expndtw0 		\cf5 \expnd0\expndtw0\kerning0
Sequential Forward Selection (SFS):
\f0\b0 \cf5 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
Sequential Forward Selection is a feature selection method used to identify a subset of features that maximizes the model's accuracy.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
A Sequential Forward Selection model (
\f1\b\fs28 \cf5 sfs
\f0\b0\fs32 \cf5 ) is created, and it iteratively adds features to the model, evaluating the accuracy at each step.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
The selected features by SFS are printed, and the model accuracy after feature selection is approximately 76.9%.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
A plot visualizing the model accuracy as a function of the number of features used is generated.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf5 \kerning1\expnd0\expndtw0 		\cf5 \expnd0\expndtw0\kerning0
Sequential Backward Selection (SBS):
\f0\b0 \cf5 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
Similar to SFS but in reverse, Sequential Backward Selection (
\f1\b\fs28 \cf5 sbs
\f0\b0\fs32 \cf5 ) starts with all features and removes them iteratively.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
Features selected by SBS, the model accuracy, and a plot of the accuracy are printed.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf5 \kerning1\expnd0\expndtw0 		\cf5 \expnd0\expndtw0\kerning0
Recursive Feature Elimination (RFE):
\f0\b0 \cf5 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
Recursive Feature Elimination is another feature selection technique where features are recursively pruned based on their importance.\
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
An RFE model (
\f1\b\fs28 \cf5 rfe
\f0\b0\fs32 \cf5 ) is created and fitted to the data, and the selected features and model accuracy are printed.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf5 \kerning1\expnd0\expndtw0 		\cf5 \expnd0\expndtw0\kerning0
Summary:
\f0\b0 \cf5 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\cf5 \expnd0\expndtw0\kerning0
The project aims to compare the model accuracy using different feature selection methods (SFS, SBS, RFE) with the accuracy obtained using all available features. The results suggest that these feature selection techniques can potentially identify a smaller subset of features while maintaining a comparable level of model accuracy.}